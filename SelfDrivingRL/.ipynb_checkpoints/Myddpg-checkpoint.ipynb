{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym_torcs import TorcsEnv\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras.engine.training import collect_trainable_weights\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "import math\n",
    "from keras.initializations import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine.training import collect_trainable_weights\n",
    "from keras.layers import Dense, Flatten, Input, merge, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck process\n",
    "\n",
    "def Ornstein_Uhlenbeck(self, x, mu, theta, sigma):\n",
    "    return theta * (mu - x) + sigma * np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        if self.num_experiences < batch_size:\n",
    "            return random.sample(self.buffer, self.num_experiences)\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        experience = (state, action, reward, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_neuron_l1 = 500\n",
    "hidden_neuron_l2 = 1000\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "\n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model , self.weights, self.state = self.create_actor_network(state_size, action_size)   \n",
    "        self.target_model, self.target_weights, self.target_state = self.create_actor_network(state_size, action_size) \n",
    "        self.action_gradient = tf.placeholder(tf.float32,[None, action_size])\n",
    "        self.params_grad = tf.gradients(self.model.output, self.weights, -self.action_gradient)\n",
    "        grads = zip(self.params_grad, self.weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(LEARNING_RATE).apply_gradients(grads)\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    def train(self, states, action_grads):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action_gradient: action_grads\n",
    "        })\n",
    "\n",
    "    def target_train(self):\n",
    "        actor_weights = self.model.get_weights()\n",
    "        actor_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(actor_weights)):\n",
    "            actor_target_weights[i] = self.TAU * actor_weights[i] + (1 - self.TAU)* actor_target_weights[i]\n",
    "        self.target_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def create_actor_network(self, state_size,action_dim):\n",
    "        print(\"building Actor model\")\n",
    "        S = Input(shape=[state_size])   \n",
    "        h0 = Dense(hidden_neuron_l1, activation='relu')(S)\n",
    "        h1 = Dense(hidden_neuron_l2, activation='relu')(h0)\n",
    "        Steering = Dense(1,activation='tanh',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1)  \n",
    "        Acceleration = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1)   \n",
    "        Brake = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1) \n",
    "        V = merge([Steering,Acceleration,Brake],mode='concat')          \n",
    "        model = Model(input=S,output=V)\n",
    "        return model, model.trainable_weights, S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For visualization\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001     #Target Network HyperParameters\n",
    "LRA = 0.0001    #Learning rate for Actor\n",
    "LRC = 0.001     #Lerning rate for Critic\n",
    "\n",
    "action_dim = 3  #Steering/Acceleration/Brake\n",
    "state_dim = 29  #of sensors input\n",
    "state_size=29\n",
    "np.random.seed(1337)\n",
    "hidden_neuron_l1 = 500\n",
    "hidden_neuron_l2 = 1000\n",
    "\n",
    "vision = False\n",
    "\n",
    "EXPLORE = 100000.\n",
    "episode_count = 20\n",
    "max_steps = 10\n",
    "reward = 0\n",
    "done = False\n",
    "step = 0\n",
    "epsilon = 1\n",
    "indicator = 0\n",
    "\n",
    "    #Tensorflow GPU optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = Input(shape=[state_size])   \n",
    "h0 = Dense(hidden_neuron_l1, activation='relu')(S)\n",
    "h1 = Dense(hidden_neuron_l2, activation='relu')(h0)\n",
    "Steering = Dense(1,activation='tanh',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1)  \n",
    "Acceleration = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1)   \n",
    "Brake = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(h1) \n",
    "V = merge([Steering,Acceleration,Brake],mode='concat')          \n",
    "model = Model(input=S,output=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='ActorModel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = Input(shape=[state_size])  \n",
    "A = Input(shape=[action_dim],name='action2')   \n",
    "w1 = Dense(hidden_neuron_l1, activation='relu')(S)\n",
    "a1 = Dense(hidden_neuron_l2, activation='linear')(A) \n",
    "h1 = Dense(hidden_neuron_l2, activation='linear')(w1)\n",
    "h2 = merge([h1,a1],mode='sum')    \n",
    "h3 = Dense(hidden_neuron_l2, activation='relu')(h2)\n",
    "V = Dense(action_dim,activation='linear')(h3)   \n",
    "Criticmodel = Model(input=[S,A],output=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(Criticmodel).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='CriticModel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CriticNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model, self.action, self.state = self.create_critic_network(state_size, action_size)  \n",
    "        self.target_model, self.target_action, self.target_state = self.create_critic_network(state_size, action_size)  \n",
    "        self.action_grads = tf.gradients(self.model.output, self.action)  #GRADIENTS for policy update\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    def gradients(self, states, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action: actions\n",
    "        })[0]\n",
    "\n",
    "    def target_train(self):\n",
    "        critic_weights = self.model.get_weights()\n",
    "        critic_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(critic_weights)):\n",
    "            critic_target_weights[i] = self.TAU * critic_weights[i] + (1 - self.TAU)* critic_target_weights[i]\n",
    "        self.target_model.set_weights(critic_target_weights)\n",
    "\n",
    "    def create_critic_network(self, state_size,action_dim):\n",
    "        print(\"building critic model\")\n",
    "        S = Input(shape=[state_size])  \n",
    "        A = Input(shape=[action_dim],name='action2')   \n",
    "        w1 = Dense(hidden_neuron_l1, activation='relu')(S)\n",
    "        a1 = Dense(hidden_neuron_l2, activation='linear')(A) \n",
    "        h1 = Dense(hidden_neuron_l2, activation='linear')(w1)\n",
    "        h2 = merge([h1,a1],mode='sum')    \n",
    "        h3 = Dense(hidden_neuron_l2, activation='relu')(h2)\n",
    "        V = Dense(action_dim,activation='linear')(h3)   \n",
    "        model = Model(input=[S,A],output=V)\n",
    "        adam = Adam(lr=self.LEARNING_RATE)\n",
    "        model.compile(loss='mse', optimizer=adam)\n",
    "        return model, A, S \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def playGame(train_indicator=1):    #1 means Train, 0 means simply Run\n",
    "    BUFFER_SIZE = 100000\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.99\n",
    "    TAU = 0.0001     #Target Network HyperParameters\n",
    "    LRA = 0.0001    #Learning rate for Actor\n",
    "    LRC = 0.001     #Lerning rate for Critic\n",
    "\n",
    "    action_dim = 3  #Steering/Acceleration/Brake\n",
    "    state_dim = 29  #of sensors input\n",
    "\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    vision = False\n",
    "\n",
    "    EXPLORE = 100000.\n",
    "    episode_count = 200\n",
    "    max_steps = 10\n",
    "    reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    epsilon = 1\n",
    "    indicator = 0\n",
    "\n",
    "    #Tensorflow GPU optimization\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    from keras import backend as K\n",
    "    K.set_session(sess)\n",
    "\n",
    "    actor = ActorNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRA)\n",
    "    critic = CriticNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRC)\n",
    "    buff = ReplayBuffer(BUFFER_SIZE)    #Create replay buffer\n",
    "\n",
    "    # Generate a Torcs environment\n",
    "    env = TorcsEnv(vision=vision, throttle=True,gear_change=False)\n",
    "\n",
    "    #Now load the weight\n",
    "    print(\"load the weight\")\n",
    "    try:\n",
    "        actor.model.load_weights(\"Myactormodel.h5\")\n",
    "        critic.model.load_weights(\"Mycriticmodel.h5\")\n",
    "        actor.target_model.load_weights(\"Myactormodel.h5\")\n",
    "        critic.target_model.load_weights(\"Mycriticmodel.h5\")\n",
    "        print(\"Weight load successfully\")\n",
    "    except:\n",
    "        print(\"Cannot find the model weight\")\n",
    "\n",
    "    print(\"TORCS RL Start...\")\n",
    "    for i in range(episode_count):\n",
    "\n",
    "        print(\"Episode : \" + str(i) + \" Replay Buffer \" + str(buff.count()))\n",
    "\n",
    "        if np.mod(i, 3) == 0:\n",
    "            ob = env.reset(relaunch=True)   #relaunch TORCS every 3 episode because of the memory leak error\n",
    "        else:\n",
    "            ob = env.reset()\n",
    "\n",
    "        s_t = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY,  ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm))\n",
    "     \n",
    "        total_reward = 0.\n",
    "        for j in range(max_steps):\n",
    "            loss = 0 \n",
    "            epsilon -= 1.0 / EXPLORE\n",
    "            a_t = np.zeros([1,action_dim])\n",
    "            noise_t = np.zeros([1,action_dim])\n",
    "            \n",
    "            a_t_original = actor.model.predict(s_t.reshape(1, s_t.shape[0]))\n",
    "            noise_t[0][0] = train_indicator * max(epsilon, 0) * Ornstein_Uhlenbeck(a_t_original[0][0],  0.0 , 0.60, 0.30)\n",
    "            noise_t[0][1] = train_indicator * max(epsilon, 0) * Ornstein_Uhlenbeck(a_t_original[0][1],  0.5 , 1.00, 0.10)\n",
    "            noise_t[0][2] = train_indicator * max(epsilon, 0) * Ornstein_Uhlenbeck(a_t_original[0][2], -0.1 , 1.00, 0.05)\n",
    "\n",
    "            #The following code do the stochastic brake\n",
    "            if random.random() <= 0.1:\n",
    "                print(\"we apply the brakes here...\")\n",
    "                noise_t[0][2] = train_indicator * max(epsilon, 0) * Ornstein_Uhlenbeck(a_t_original[0][2],  0.2 , 1.00, 0.10)\n",
    "\n",
    "            a_t[0][0] = a_t_original[0][0] + noise_t[0][0]\n",
    "            a_t[0][1] = a_t_original[0][1] + noise_t[0][1]\n",
    "            a_t[0][2] = a_t_original[0][2] + noise_t[0][2]\n",
    "\n",
    "            ob, r_t, done, info = env.step(a_t[0])\n",
    "\n",
    "            s_t1 = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY, ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm))\n",
    "        \n",
    "            buff.add(s_t, a_t[0], r_t, s_t1, done)      #Add replay buffer\n",
    "            \n",
    "            #Do the batch update\n",
    "            batch = buff.getBatch(BATCH_SIZE)\n",
    "            states = np.asarray([e[0] for e in batch])\n",
    "            actions = np.asarray([e[1] for e in batch])\n",
    "            rewards = np.asarray([e[2] for e in batch])\n",
    "            new_states = np.asarray([e[3] for e in batch])\n",
    "            dones = np.asarray([e[4] for e in batch])\n",
    "            y_t = np.asarray([e[1] for e in batch])\n",
    "\n",
    "            target_q_values = critic.target_model.predict([new_states, actor.target_model.predict(new_states)])  \n",
    "           \n",
    "            for k in range(len(batch)):\n",
    "                if dones[k]:\n",
    "                    y_t[k] = rewards[k]\n",
    "                else:\n",
    "                    y_t[k] = rewards[k] + GAMMA*target_q_values[k]\n",
    "       \n",
    "            if (train_indicator):\n",
    "                loss += critic.model.train_on_batch([states,actions], y_t) \n",
    "                a_for_grad = actor.model.predict(states)\n",
    "                grads = critic.gradients(states, a_for_grad)\n",
    "                actor.train(states, grads)\n",
    "                actor.target_train()\n",
    "                critic.target_train()\n",
    "\n",
    "            total_reward += r_t\n",
    "            s_t = s_t1\n",
    "        \n",
    "            print(\"Episode\", i, \"Step\", step, \"Action\", a_t, \"Reward\", r_t, \"Loss\", loss)\n",
    "        \n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if np.mod(i, 3) == 0:\n",
    "            if (train_indicator):\n",
    "                print(\"Now we save model\")\n",
    "                actor.model.save_weights(\"Myactormodel.h5\", overwrite=True)\n",
    "                with open(\"Myactormodel.json\", \"w\") as outfile:\n",
    "                    json.dump(actor.model.to_json(), outfile)\n",
    "\n",
    "                critic.model.save_weights(\"Mycriticmodel.h5\", overwrite=True)\n",
    "                with open(\"Mycriticmodel.json\", \"w\") as outfile:\n",
    "                    json.dump(critic.model.to_json(), outfile)\n",
    "\n",
    "        print(\"TOTAL REWARD @ \" + str(i) +\"-th Episode  : Reward \" + str(total_reward))\n",
    "        print(\"Total Step: \" + str(step))\n",
    "        print(\"\")\n",
    "\n",
    "    env.end()  # This is for shutting down TORCS\n",
    "    print(\"Finish.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    playGame()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
